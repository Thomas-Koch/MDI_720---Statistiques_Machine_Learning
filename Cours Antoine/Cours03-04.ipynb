{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cours 3 et 4 - Propriétés de l'estimateur des moindres carrés\n",
    "\n",
    "23 et 30 septembre 2019, François Portier\n",
    "\n",
    "## Programme\n",
    "\n",
    "- Moindres carrés et théorème de projection\n",
    "- Centralisation et Normalisation\n",
    "- Prop des MCO (OLS) dans le cas du modèle \"fixed design\" (entrées non stochastiques)\n",
    "- Prop des MCO dans le cas du modèle sous-gaussien\n",
    "- Prop des MCO dans le cas du modèle gaussien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Théorème de projection\n",
    "\n",
    "Théorème [Hilbert|\n",
    "\n",
    "Soit $F \\subset \\mathbb{R}^n$ un sous-espace vectoriel, soit $y \\in \\mathbb{R}^n$\n",
    "\n",
    "Soit $ \\underset{z \\in F}{inf} \\Vert y - z \\Vert^2$ de point $z \\in F$ est atteint de façon unique.\n",
    "\n",
    "Il est caractérisé par les équations normales : $\\langle y-z, f\\rangle = 0, \\forall f \\in F$\n",
    "\n",
    "(SCHEMA Projection y en z sur le plan défini par F)\n",
    "\n",
    "Référence : https://fr.wikipedia.org/wiki/Théorème_de_projection_sur_un_convexe_fermé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application au MCO\n",
    "- $y \\in \\mathbb{R}^n$\n",
    "- $ F = Vect(Xh)$ \n",
    "- Le vecteur z s'écrit $ z^* = X \\hat{\\theta_n}$ \n",
    "- Nous est donné par $\\langle Y - X\\hat{\\theta_n}, f \\rangle = 0, \\forall f \\in F$\n",
    "- $ \\iff  (Y - X\\hat{\\theta_n})^TX = 0$ (REM: SSI ou implique ?)\n",
    "- $ \\iff X^TX\\hat{\\theta_n} = X^T Y$\n",
    "\n",
    "On retrouve le résultat obtenu par la minimisation gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Théorème\n",
    "\n",
    "Soit $y\\in R^n$, $x \\in R$\n",
    "\n",
    "L'estimateur des MCO existe. \n",
    "On a 2 cas :\n",
    "- Si $Ker(X) = \\{0\\} $ \n",
    "    - Alors $\\hat{\\theta_n}$. est unique \n",
    "    - Car $ Ker(X) = Ker(X^TX) \\iff$ (EqNormal) unique\n",
    "\n",
    "\n",
    "- Si $Ker(X) \\ne \\{0\\} $ \n",
    "    - Alors le système possède une infinité de solution\n",
    "\n",
    "Ex. $\\hat{\\theta_n}$ est solution de (EqNormale), Soit $ u \\in Ker(X)$, alors $\\forall \\lambda \\in R$, on a  $\\hat{\\theta_n} + \\lambda u$ est solution (voir Cours02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention $X \\hat{\\theta_n}$ est _unique_ car c'est l'infimum dans le théorème de projection\n",
    "\n",
    "Le projecteur est unique, mais le vecteur permettant de l'atteindre n'est pas unique\n",
    "\n",
    "\n",
    "La non unicité est un problème d'identifiabilité causé par la structure de la matrice de design X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Normalisation et \"centralisation\" des données\n",
    "\n",
    "Le pb des MCO est souvent introduit de la façon suivante:\n",
    "$$ Y \\in R^n, \\tilde{X} \\in R^{n \\times p}$$\n",
    "\n",
    "$$ min_{\\theta \\in R^p} \\Vert Y - \\mathbb{1} \\tilde{\\theta_0} - \\tilde{X} \\tilde{\\theta} \\Vert^2_2, (1)$$ \n",
    "\n",
    "En notant : $X = ( \\mathbb{1}, \\tilde{X})$, $ \\theta = ( \\tilde{\\theta_0}, \\tilde{\\theta} ) $ on obtient la formulation:\n",
    "\n",
    "$$ min_{\\theta \\in R^{p+1}} \\Vert Y - X \\theta \\Vert^2_2 $$\n",
    "\n",
    "C'est la régression avec intercept\n",
    "\n",
    "\n",
    "Prédiction:\n",
    "$$ P_1(\\tilde{X_c}) =\\tilde{\\theta}^T \\tilde{X_c} + \\tilde{\\theta_0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut envisager aussi le programme suivant:\n",
    "\\begin{cases}\n",
    "Y_c &= Y - \\mathbb{1} \\overline{Y} \\\\\n",
    "\\tilde{X_c} &= \\tilde{X} - \\mathbb{1} \\overline{\\tilde{X}}^T\\\\\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCO devient:\n",
    "\n",
    "$$min_{\\tilde{\\theta} \\in R^p} \\Vert Y_c - \\tilde{X_c} \\tilde{\\theta} \\Vert_2^2, (2)$$ \n",
    "\n",
    "Prédiction:\n",
    "\n",
    "$$ P_2(\\tilde{X_c}) =\\tilde{\\theta}^T (\\tilde{X_c} - \\overline{\\tilde{X_c}}) + \\overline{Y}$$\n",
    "\n",
    "(!! ou : $ P_2(\\tilde{X}) =\\tilde{\\theta}^T (\\tilde{X} - \\overline{\\tilde{X}}) + \\overline{Y}$ ??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pour le TD \n",
    "\n",
    "Montrer :\n",
    "\\begin{align}\n",
    " min_{\\theta \\in R^{p+1}} \\Vert Y - X \\theta \\Vert_2^2 \n",
    " &= min_{\\tilde{\\theta} \\in R^{p}} \\Vert Y_c - \\tilde{X_c} \\tilde{\\theta} \\Vert_2^2 \\\\\n",
    " &= min_{\\tilde{\\theta} \\in R^{p}} \\Vert Y - (\\tilde{X_c} \\tilde{\\theta} + \\mathbb{1} \\overline{Y}) \\Vert_2^2 \n",
    " \\end{align}\n",
    "\n",
    "(Admis pour le moment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le problème (1) s'écrit directement dans le même cadre que le théorème de projection\n",
    "\n",
    "Donc : $ \\hat{Z} = X \\hat{\\theta}$ est l'unique point de $Vect(X)$ qui minimise $ \\Vert Y - \\hat{Z} \\Vert_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais comme :\n",
    "$$\\begin{align}\n",
    "\\tilde{X_c} \\tilde{\\theta_n} + \\mathbb{1} \\overline{Y} \n",
    "&= \\tilde{X_c} \\tilde{\\theta_n} - \\mathbb{1} \\overline{\\tilde{X_n}} \\tilde{\\theta_n} + \\mathbb{1} \\overline{Y} \\\\\n",
    "&= (\\mathbb{1}, \\tilde{X}) {\\overline{Y} - \\overline{\\tilde{X_n}} \\tilde{\\theta_n} \\choose \\tilde{\\theta_n} }\n",
    "\\end{align}$$\n",
    "\n",
    "Par unicité du project (dans le théorème), on a que : \n",
    "$$ \\hat{Z} = X \\tilde{\\theta_n} \n",
    "= X { \\overline{Y} - \\overline{\\tilde{X_n}} \\tilde{\\theta_n} \\choose \\tilde{\\theta_n} } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si $Ker(X) = \\{0\\}$ alors: \n",
    "$$ \\hat{\\theta_n} = { \\overline{Y} - \\overline{\\tilde{X_n}} \\tilde{\\theta_n} \\choose \\tilde{\\theta_n} }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conseils et commentaires:\n",
    "- Ne pas oublier d'estimer l'intercept\n",
    "- Les pénalisation Lasso et Ridge ne se font jamais sur la moyenne\n",
    "\n",
    "Si le $Ker(X)$ est différent de {0}, les prédicteurs sont les mêmes sur l'espace $X$, mais inconnu sur un autre espace (ex. : nouvelle donnée)\n",
    "\n",
    "Grace à l'équation précédente :\n",
    "$$ \\forall \\tilde{x} \\in \\mathbb{R}^p,  P_1(\\tilde{x}) = P_2(\\tilde{x}) $$\n",
    "\n",
    "$$  P_1(\\tilde{x}) = \\sum_{k=0}^p \\hat{\\theta_{k,n}}\\tilde{x_k} + \\tilde{\\theta_{0,n}}$$\n",
    "\n",
    "$$  P_2(\\tilde{x}) = \\sum_{k=0}^p \\hat{\\theta_{k,n}}(\\tilde{x_k} - \\overline{\\tilde{X_n}}) + \\overline{Y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Normalisation\n",
    "\n",
    "Supposons que X et Y sont centrées\n",
    "\n",
    "$ \\hat{\\theta_a} \\in argmin_{\\theta \\in R^p} \\Vert Y - X A \\theta \\Vert_2^2$ avec $A \\in R^{p\\times p}$ inversible\n",
    "\n",
    "Par inversibilité de la matrice A:\n",
    "$ \\Vert Y - X A \\hat{\\theta_A} \\Vert_2^2 = \\Vert Y - X \\hat{\\theta} \\Vert_2^2 $\n",
    "\n",
    "Avec $ \\hat{\\theta} \\in {argmin}_{\\theta \\in R^p} \\Vert Y - X \\theta \\Vert_2^2$\n",
    "\n",
    "Et donc par le théo de projection : \n",
    "$ X A \\hat{\\theta_a} = X \\hat{\\theta}$\n",
    "\n",
    "Donc, Si $Ker(X) = \\{0\\}$  \n",
    "\n",
    "$ A \\hat{\\theta_a} = \\hat{\\theta} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, les 2 prédicteurs associés coincident.\n",
    "\n",
    "En pratique, procédure de recentrage : \n",
    "\\begin{cases}\n",
    "Y_c &= Y - \\overline{Y} \\\\\n",
    "\\tilde{X_c} &= X - ( \\mathbb{1} \\overline{X}^T) \\\\\n",
    "\\end{cases}\n",
    "\n",
    "Rescaling:\n",
    "$$ \\tilde{v_k} = \\frac 1n \\sum_i \\left(\\tilde{X_{k,i}} - \\overline{\\tilde{X_{k,i}}} \\right)^2$$\n",
    "\n",
    "$$ X = \\tilde{X_c} . ( \\frac{1}{\\sqrt{\\tilde{v_1}}}... \\frac{1}{\\sqrt{\\tilde{v_p}}})^T Idp $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : le fait de ne pas changer de base (rotation) permet de conserver l'interprétabilité. Cela correspond au fait que A doit être diagonale.\n",
    "\n",
    "Différence entre normalisation et standardisation ?\n",
    "\n",
    "Standardisé = centré-réduite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) Prop des MCO (OLS) dans le cas du modèle \"fixed design\"\n",
    "\n",
    "Fixed design = entrées non stochastiques \n",
    "\n",
    "Modèle : Soit $Y \\in R^n, X \\in R^{n \\times p}$, avec $X = (\\mathbb{1}, \\tilde{X})$\n",
    "\n",
    "$$ \\hat{\\theta_n}\\in min_{\\theta \\in R^{p+1}} \\Vert Y - X \\theta \\Vert_2^2$$\n",
    "\n",
    "2 cas possibles : \n",
    "- $ (\\overline{Y}, \\overline{X} ) = 0$ pas d'intercept\n",
    "- $ (\\overline{Y}, \\overline{X}) \\ne 0$ intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On suppose que :\n",
    "$$ Y = X \\theta^* + \\epsilon$$\n",
    "\n",
    "avec \n",
    "- $ \\theta^* \\in \\mathbb{R}^{p+1}$\n",
    "- $\\epsilon$ le bruit du modèle tq c'est un vecteur aléatoire à valeur dans $\\mathbb{R}^n$ : $\\begin{cases}\n",
    "Cov(\\epsilon) &= \\sigma^2 Id_n\\\\ \n",
    "E(\\epsilon) &= 0\n",
    "\\end{cases}$\n",
    "\n",
    "Commentaire : Ce qui correspond à l'indépendance des $\\epsilon$ et donc des Y:\n",
    "\n",
    "__1er point__ : $Cov(\\epsilon)$ diagonale est très liée à l'idée d'indépendance dans les sorties du modèle $(Y_i), i \\in (1..n)$\n",
    "\n",
    "__2eme point__ : $\\sigma^2$ constant correspond à l'idée de même distribution dans les données  ou plutôt même niveau de bruit. (stationarité)\n",
    "\n",
    "__Attention__ : la matrice de X est non-stochastique. Les variables vecteurs aléatoires sont Y et $\\epsilon$\n",
    "\n",
    "\n",
    "Note/question : comment vérifier si la covariance est constante ? Voir test de White. En pratique, vérification sur les résidus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple : Expérience dans un laboratoire avec environnement contrôlé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposition : [Biais et variance]: \n",
    "\n",
    "Sous le modèle fixed design et $Ker(X) = \\{0\\}$\n",
    "- $Biais(\\hat{\\theta_n}) = E(\\hat{\\theta_n}) - \\theta^* = 0$\n",
    "- $Cov(\\hat{\\theta_n}) = \\sigma^2 (X^TX)^{-1} = \\frac 1n \\sigma^2 \\hat{G}^{-1}$\n",
    "\n",
    "avec $ \\hat{G} = \\frac 1n (X^TX)$ la matrice de Gram\n",
    "\n",
    "Rem : \n",
    "- a) $ \\hat{G} = \\frac 1n \\sum_i x_i x_i^T$\n",
    "En tant que moyenne, on espère qu'elle converge vers une limite (voir random design)\n",
    "- b) on retrouve la vitesse habituelle en $ 1/\\sqrt{n}$ du TCL\n",
    "    - $ \\sqrt{n} (\\frac 1n \\sum(z_i - E(z_i))) \\rightarrow \\mathcal{N}(0, v)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "Var(\\hat{\\mu_n} - \\mu) &= Var \\left( \\frac 1n \\sum_i Z_i - E(Z_i) \\right) \\\\\n",
    "& = \\frac{1}{n^2} E \\left( \\left(\\sum_i Z_i - E(Z_i)\\right)^2 \\right) \\\\\n",
    "& = \\frac{1}{n^2} E \\left( \\sum_{i,j} \\left(Z_i - E(Z_i)\\right) \\left(Z_j - E(Z_j)\\right)\\right) \\\\\n",
    "& = \\frac{1}{n^2} E \\left( \\sum_i \\left(Z_i - E(Z_i)\\right)^2 + \\sum_{i \\ne j} \\left(Z_i - E(Z_i)\\right) \\left(Z_j - E(Z_j)\\right)\\right)  \\\\\n",
    "\\end{align}\n",
    "\n",
    "Si indep : \n",
    "\\begin{align}\n",
    "& = \\frac{1}{n^2} \\sum_i E \\left((Z_i - E(Z_i))^2 \\right) \n",
    "    + \\sum_{i \\ne j} E \\left(Z_i - E(Z_i) \\right) E\\left(Z_j - E(Z_j) \\right) \\\\\n",
    "& = \\frac{1}{n^2} \\sum_i E \\left((Z_i - E(Z_i))^2 \\right) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Si identiquement distribuées:\n",
    "\\begin{align}\n",
    "= \\frac 1n Var(Z_1)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preuve : \n",
    "On utilise la décomposition suivante : \n",
    "\\begin{align}\n",
    "\\hat{\\theta_n} - \\theta^* &= (X^T X)^{-1} X ^t Y - \\theta^* \\\\\n",
    "&= (X^T X)^{-1} X^T \\epsilon\n",
    "\\end{align}\n",
    "\n",
    "Car:\n",
    "$ Y= X \\theta^* + \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Espérance (Biais):\n",
    "$$E(\\hat{\\theta_n} - \\theta^* ) = A_X E(\\epsilon) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "Cov(\\hat{\\theta_n} ) &= Cov(\\hat{\\theta_n} - \\theta^*) \\\\\n",
    "&= Cov(A_x \\epsilon) \\\\\n",
    "&= A_x Cov(\\epsilon) A_x^T \\\\\n",
    "&= A_x \\sigma^2 Id_n A_x^T \\\\\n",
    "&= \\sigma^2 A_x A_x^T \\\\\n",
    "&= \\sigma^2 (X^TX)^{-1}\n",
    "\\end{align}\n",
    "\n",
    "Note : l'inverse d'une matrice symmétrique est symétrique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance du biais de la variance:\n",
    "\n",
    "On retrouve ces 2 quantités dans la décomposition du risque Quadratique : \n",
    "\\begin{align}\n",
    "R_{quad}( \\hat{\\theta_n} ) &= E( \\Vert \\hat{\\theta_n} - \\theta^* \\Vert_2^2 ) \\\\\n",
    "&= E( \\Vert \\hat{\\theta_n} - E(\\hat{\\theta_n}) + E(\\hat{\\theta_n}) - \\theta^* \\Vert_2^2 ) \\\\\n",
    "&= E( \\Vert \\hat{\\theta_n} - E(\\hat{\\theta_n}) \\Vert_2^2) + E( \\Vert E(\\hat{\\theta_n}) - \\theta^* \\Vert_2^2 ) + 2 E( \\langle\\hat{\\theta_n} - E(\\hat{\\theta_n}) , E(\\hat{\\theta_n}) - \\theta^* \\rangle )\\\\\n",
    "&= E( \\Vert \\hat{\\theta_n} - E(\\hat{\\theta_n}) \\Vert_2^2 ) + E( \\Vert E(\\hat{\\theta_n}) - \\theta^* \\Vert_2^2 ) + 2  \\langle E(\\hat{\\theta_n} - E(\\hat{\\theta_n})) , E(\\hat{\\theta_n}) - \\theta^* \\rangle \\\\\n",
    "\\end{align}\n",
    "\n",
    "Hors: \n",
    "$ E(\\hat{\\theta_n} - E(\\hat{\\theta_n})) = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intermède de la trace\n",
    "\n",
    "Si $ U \\in \\mathbb{R}^{n \\times n} $ alors : $ \\Vert U \\Vert_F^2 = \\sum_{i = 0}^{n} \\sum_{j = 0}^{n} U_{i, j}^2 = tr( U U^T) $\n",
    "(norme de Frobius)\n",
    "\n",
    "Si $ u \\in \\mathbb{R}^{n} $ alors : $ \\Vert u \\Vert_2^2 = \\sum_{i = 0}^{n} u_i^2 = tr( u u^T) $\n",
    "(norme Euclienne)\n",
    "\n",
    "Trace :\n",
    "- $ tr(M) = \\sum_i m_{i,i} $\n",
    "- $ tr(AB) = tr(BA) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "R_{quad}( \\hat{\\theta_n} ) &= E \\left( tr\\left(\\left(\\hat{\\theta_n} - E(\\hat{\\theta_n})\\right) \n",
    "\\left(\\hat{\\theta_n} - E(\\hat{\\theta_n})\\right)^T\\right)\\right)\n",
    " + \\Vert Biais(\\hat{\\theta_n}) \\Vert_2^2  \\\\\n",
    "&= tr\\left( E \\left(\\left(\\hat{\\theta_n} - E(\\hat{\\theta_n})\\right) \n",
    "\\left(\\hat{\\theta_n} - E(\\hat{\\theta_n})\\right)^T\\right)\\right)\n",
    " + \\Vert Biais(\\hat{\\theta_n}) \\Vert_2^2  \\\\\n",
    "&= tr \\left( Cov(\\hat{\\theta_n}) \\right) + \\Vert Biais (\\hat{\\theta_n}) \\Vert_2^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre cas : \n",
    "\\begin{align}\n",
    "R_{quad}( \\hat{\\theta_n} ) &= \\sigma^2 tr \\left( (X^T X)^{-1} \\right) \\\\\n",
    "&= \\frac 1n \\sigma^2  tr \\left( \\hat{G}^{-1}\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme d'habitude le bruit du modèle $\\sigma^2$ et le nombre d'exemples influencent la perf.\n",
    "\n",
    "Problème du conditionnement de $\\hat{G}$ ou de X:\n",
    "\\begin{align} \n",
    "tr( \\hat{G}^{-1}) &= tr( U D^{-1} U^T)\n",
    "&= \\sum_i \\lambda_k^{-1}\n",
    "\\end{align} \n",
    "\n",
    "Par décomposition spetrale de G. $\\lambda_k$ sont les valeurs propres de $\\hat{G}$\n",
    "\n",
    "Si une valeur propre est très petite, on a un problème de conditionnement\n",
    "\n",
    "Donc s'il y a des fortes co-linéarités parmis les vecteurs de X, alors la performance de $\\hat{\\theta_n}$ sera réduite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risque prédictif\n",
    "\n",
    "Soit $ Y^* = X \\theta^*$ (Y sans le bruit)\n",
    "\n",
    "\\begin{align}\n",
    "R_{pred} \\left(\\hat{\\theta_n} \\right) &= E \\left( \\Vert Y^* - \\hat{y} \\Vert_2^2 \\right) \\\\\n",
    "&= E \\left( \\Vert X ( \\hat{\\theta_n} - \\theta^* ) \\Vert_2^2 \\right) \\\\\n",
    "&= E \\left( \\Vert X (X^TX)^{-1} X^T \\epsilon \\Vert_2^2 \\right) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Soit $ H_x = X (X^TX)^{-1} X^T $. \n",
    "\n",
    "On remarque que $ H_x$ est un projecteur orthogonal et on écrit : \n",
    "\\begin{align}\n",
    "R_{pred}(\\hat{\\theta_n} ) &= E \\left( \\Vert \\epsilon^T H_x \\epsilon \\Vert_2^2 \\right)\\\\\n",
    "&= E \\left( tr\\left( H_x \\epsilon^T \\epsilon \\right)\\right)\\\\\n",
    "&= tr \\left( H_x E\\left(  \\epsilon^T \\epsilon \\right)\\right)\\\\\n",
    "&= \\sigma^2 tr \\left(H_x \\right)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question pour le quiz\n",
    "\n",
    "$$ Ker((\\mathbb{1}, \\tilde{X})) = \\{0\\} \\iff Ker(\\hat{\\Sigma)} = \\{0\\} $$\n",
    "\n",
    "$$ \\hat{\\Sigma} = \\frac 1n \\sum_{i=1}^n (\\hat{x_i} - \\overline{X}) (\\hat{x_i} - \\overline{X})^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(REPRISE lundi 30 septembre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme $ Y = X \\theta^* + \\epsilon $:\n",
    "\\begin{align}\n",
    "(Id_n - H_x) Y \n",
    "&= (Id_n - H_x)(X \\theta^* + \\epsilon) \\\\\n",
    "&= (Id_n - H_x)\\epsilon\n",
    "\\end{align}\n",
    "\n",
    "En effet : $ X\\theta^* \\in Vect(X) = Im(H_x) = Ker(Id_n - H_x) $ car $(Id_n - H_x)$ est un projecteur sur l'espace complémentaire de $Im(H_x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exo\n",
    "\n",
    "Prouver que $H_x$ est un projecteur orthogonal, prouver que $Im(H_x) = Vect(X)$\n",
    "\n",
    "Projecteur orthogonal :\n",
    "1. Projecteur ssi indepotent : $ H_x.H_x = X(X^TX)^{-1}X^T.X(X^TX)^{-1}X^T = X(X^TX)^{-1}X = H_x $\n",
    "2. Orthogonal ssi endomorphisme symétrique : $ H_x^T = (X^T)^T ((X^TX)^{-1})^T X^T = X(X^TX)^{-1}X^T$, en utilisant que l'inverse d'une matrice symétrique est aussi symétrique.\n",
    "\n",
    "Projecteur sur Vect(X) :\n",
    "Selon le théorème du projeteur le projeté z de y est atteint de façon unique et respecte : $\\langle y-z, f\\rangle = 0, \\forall f \\in F$\n",
    "\n",
    "Si on prend $F = Vect(Xh)$ et $Z = X \\hat{\\theta}$, on obtient $Z = H_x Y $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimateur du niveau de bruit $\\sigma^2$\n",
    "\n",
    "Posons:\n",
    "\\begin{align}\n",
    "\\tilde{\\sigma^2} \n",
    "&= \\mathbb{E} \\left( \\Vert (Id_n - H_x) \\epsilon \\Vert_2^2 \\right) \\\\\n",
    "&= tr \\left( \\mathbb{E} \\left( (Id_n - H_x) \\epsilon \\epsilon^T (Id_n - H_x) \\right) \\right) \\\\\n",
    "&= \\sigma^2 tr \\left(Id_n - H_x \\right) \\\\\n",
    "&= \\sigma^2 (n - (p+1)) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Car $dim(Im(Id_n - H_x)) = n - dim(Ker(Id_n - H_x)) = n - dim(Vect(X)) = n - (p+1)$ (l'intercept a été intégré à X)\n",
    "\n",
    "Cet estimateur est donc biaisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'estimateur non biaisé est:\n",
    "$$ \\sigma^2 = \\frac{1}{n - p -1} \\sum_{i = 1}^n \\hat{\\epsilon_i}^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque, si on prend le modèle sans intercept :\n",
    "\\begin{align}\n",
    "\\forall i, \\hat{\\epsilon_i} \n",
    "&= (Y_i - \\overline{Y}) - (\\tilde{X_i} - \\overline{\\tilde{X}})\\tilde{\\theta_n} \\\\\n",
    "&= Y_i - \\left( \\tilde{X_i} \\tilde{\\theta_n}  + (\\overline{Y} - \\overline{\\tilde{X}}\\tilde{\\theta_n}) \\right) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Le terme entre parenthêse est égale à $ X_i \\hat{\\theta_n} $ (voir cours 03 + à revoir en TD)\n",
    "\n",
    "Conclusion, même avec $\\tilde{X} \\in \\mathbb{R}^{n \\times p}$ il faut diviser par $(n-p-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas de la réduction:\n",
    "\\begin{align}\n",
    "\\hat{Y} &= X \\hat{\\theta_n} \\\\\n",
    "&= X D \\tilde{\\theta_n} \\\\\n",
    "&= X D D^{-1} \\hat{\\theta_n} \\\\\n",
    "&= X \\hat{\\theta_n} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Il n'y a donc aucun effet sur la prédiction. L'effet est uniquement sur les coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d) Modèle sous-gaussien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition : \n",
    "\n",
    "$ Y = X \\theta^* + \\epsilon $ avec : $\\begin{cases}\n",
    "Y &\\in \\mathbb{R}^{n} \\\\\n",
    "X &\\in \\mathbb{R}^{n \\times (p+1)}\n",
    "\\end{cases}$\n",
    "\n",
    "Et $\\epsilon$ tel que $\\epsilon_i$ sont des variables aléatoires $E(\\epsilon) = 0$, indépendantes et sous-gaussiennes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de sous-gaussien\n",
    "\n",
    "Une variable centrée Z est dite sous gaussienne si :\n",
    "$$ \\forall \\lambda \\in \\mathbb{R}, \\mathbb{E}\\left( e^{\\lambda Z}\\right) \\le e^{\\frac{\\lambda^2 \\sigma^2}{2}} $$\n",
    "\n",
    "Note : Correspond à la transformée de Laplace de Z et de la Gaussienne (https://en.wikipedia.org/wiki/Sub-Gaussian_distribution), équivalent à : $ \\exists C,v / P(Z > t) \\le e^{-vt^2}$ \n",
    "\n",
    "Prendre des moments exponentiels implique que la loi ne prend pas de grandes valeurs. Les valeurs sont concentrées autour de la moyenne.\n",
    "\n",
    "Exemple : \n",
    "- variables gaussiennes\n",
    "- variables bornées (loi uniforme)\n",
    "- variables Student\n",
    "\n",
    "Comme montré ensuite, l'hypothèse permet d'obtenir des inégalités de concentrations plus intéressantes que le risque quadratic.\n",
    "\n",
    "Note : l'hypothèse minimum utilisé jusqu'ici pour le moment d'ordre p des lois est l'espace : \n",
    "$$ L^p(\\Omega) = \\{ x: \\Omega \\rightarrow \\mathbb{R} / E(x^2) < \\infty \\} $$\n",
    "$$ \\implies L^p(\\Omega) \\subset L^{p-1}(\\Omega) ...\\subset L^1(\\Omega)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cas de la moyenne\n",
    "Soit $Z_i$ une suite de variables aléatoires indépendantes centrées sous gaussiennes de paramètres $\\sigma_i^2$\n",
    "\n",
    "D'après le résultat précédent:\n",
    "$$\\begin{align}\n",
    "E \\left((\\frac 1n \\sum_i Z_i)^2 \\right) \n",
    "&= \\frac{1}{n^2} \\sum_i Var(Z_i) \\\\\n",
    "&\\approx \\frac 1n \\sigma^2\n",
    "\\end{align}$$\n",
    "Si $\\forall i, \\sigma_i = \\sigma $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On recherche une fonction b tel que:\n",
    "$$ \\forall t  >0, P \\left(  \\vert \\sum_{i=1}^n Z_i \\vert > t \\right) \\le b \\left(t, (\\sigma_i)_{i \\in 1..n}, n \\right) $$\n",
    "\n",
    "(Note de rédaction : $\\frac 1n$ retiré directement)\n",
    "\n",
    "Quelle est la décroissance en t ?\n",
    "\n",
    "Rappels sur la fonction de répartition et les inégalités classiques :\n",
    "$$\\begin{align}\n",
    "P \\left(  \\vert \\sum_i Z_i \\vert > t \\right)\n",
    "&= P \\left(  \\vert S_n \\vert > t \\right) \\\\\n",
    "& = \\mathbb{P}(A) = \\int_A dP = \\int \\mathbb{1}_A dp \\\\\n",
    "&= E(\\mathbb{1}_A)\n",
    "\\end{align}$$\n",
    "\n",
    "Avec : \n",
    "- $ S_n = \\sum_{i=1}^n Z_i $ est une variable aléatoire positive\n",
    "- $ \\mathbb{1}_A $ est la fonction indicatrice qui est égale à 1 sur l'ensemble A\n",
    "- $ A = \\{\\vert S_n \\vert > t \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'inégalité de Markov s'applique :\n",
    "    \n",
    "$$ E(\\mathbb{1}_{S_n > t}) \\le \\frac{E( \\vert S_n \\vert )}{t} $$\n",
    "\n",
    "Note : correspond à majorer la fonction échelon $\\mathbb{1}_{ \\vert  S_n \\vert > t}$ par la droite qui passe par l'origine et (t, 1)\n",
    "\n",
    "(https://fr.wikipedia.org/wiki/Inégalité_de_Markov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En appliquant l'inégalité de Markov au moment d'ordre 2, soit [l'inégalité de Bienaymé-Tchebychev](https://fr.wikipedia.org/wiki/Inégalité_de_Bienaymé-Tchebychev):\n",
    "\n",
    "$$ E(\\mathbb{1}_{S_n > t}) \\le \\frac{E(S_n^2)}{t^2} $$\n",
    "\n",
    "Conclusion : plus les variables aléatoires auront des moment élevés, plus la borne sera meilleure en t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouvons une meilleure borne dans le cas sous-gaussien.\n",
    "\n",
    "En appliquant l'inégalité de Markov à la v.a. positive $e^{\\lambda X}$ on obtient ([Chernoff](https://en.wikipedia.org/wiki/Chernoff_bound)):\n",
    "$$ \\forall \\lambda > 0, P(X > t) = P(e^{\\lambda X} > e^{\\lambda t}) = \\mathbb{1}_{e^{\\lambda X} > e^{\\lambda t}} \\le E\\left( e^{\\lambda (X - t)}\\right) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donc: \n",
    "\n",
    "$$\\begin{align}\n",
    "P(\\vert S_n \\vert > t ) \\le E\\left( e^{\\lambda (S_n - t )} \\right) \n",
    "&= E\\left( e^{\\lambda S_n} \\right) e^{-\\lambda t} \\\\\n",
    "&= E\\left( e^{\\lambda S_n} \\right) e^{-\\lambda t} \\\\ \n",
    "&= E\\left( e^{ \\sum_{i=0}^{n} \\lambda Z_i} \\right) e^{-\\lambda t} \\\\ \n",
    "&= E\\left( \\prod_{i=0}^{n} e^{\\lambda Z_i} \\right) e^{-\\lambda t} \\\\ \n",
    "&= \\prod_{i=0}^{n} E\\left( e^{\\lambda Z_i} \\right) e^{-\\lambda t} , (1)\\\\ \n",
    "&\\le \\prod_{i=0}^{n} E\\left( e^{\\lambda^2 \\sigma_i^2 / 2} \\right) e^{-\\lambda t} , (2) \\\\\n",
    "&\\le E\\left( e^{\\lambda^2 v_n / 2 -\\lambda t} \\right), v_n = \\sum_{i=0}^{n} \\sigma_i^2\n",
    "\\end{align}$$\n",
    "\n",
    "- (1) par indépendance\n",
    "- (2) par hypothèse sous-gaussienne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimisation de $\\lambda^2 v_n /2 - \\lambda t$, en $\\lambda$ :\n",
    "$$\\begin{align}\n",
    "&\\implies \\lambda^* v_n - t = 0 \\\\\n",
    "&\\implies \\lambda^*  =  \\frac{t}{v_n}\n",
    "\\end{align}$$\n",
    "\n",
    "D'où : \n",
    "$$\\begin{align}\n",
    "P(\\vert S_n \\vert > t ) &\\le E\\left( e^{\\frac{t^2}{2v_n} - \\frac{t^2}{v_n}} \\right) \\\\\n",
    "&\\le E\\left( e^{-\\frac{t^2}{2v_n}} \\right)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce résultat est important car il est valable pour tout n et non pour une asymptote comme dans le cas du TCL.\n",
    "\n",
    "De façon équivalente (en ) : pour tout $n \\ge 1$ et $\\delta \\in ]0, 1[$ on a avec proba $(1-\\delta)$ : \n",
    "\n",
    "$$ \\vert S_n \\vert \\le \\sqrt{2 v_n log \\left(\\frac 2\\delta\\right )}, (*)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application au MCO\n",
    "\n",
    "$$ (\\hat{\\theta_n} - \\theta^* ) = (X^T X)^{-1} X^Z \\epsilon $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition :\n",
    "$$\\begin{align} S_{n,k} \n",
    "&= (\\hat{\\theta_{n,k}} - \\theta_k^* ) = e_k^T (X^T X)^{-1} X^Z \\epsilon, e_k^T = (0,0,...,1,0,...,0) \\\\\n",
    "&= \\sum_{i=0}^n \\alpha_{i,k} \\epsilon_i, \\alpha_{i,k} = e_k^T (X^T X)^{-1} X_i\n",
    "\\end{align}$$\n",
    "\n",
    "Propriété : $\\alpha_{i,k} \\epsilon_i$ est une sous-gaussienne de variance $\\alpha_{i,k}^2 \\sigma^2$\n",
    "\n",
    "Preuve : $ E(e^{\\lambda \\alpha_{i,k}^2 \\epsilon_i}) \\le e^{\\lambda \\alpha_{i,k} \\sigma^2}$, $\\epsilon_i$ sous-gaussienne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En appliquant $(*)$, on a avec probabilité $(1- \\delta)$ : \n",
    "$$ \\vert \\hat{\\theta_{n,k}} - \\theta_k^* \\vert \n",
    "\\le \\sqrt{2 \\sigma^2 \\sum_{i = 1}^{n} \\alpha_{i,k} log \\left(\\frac 2\\delta\\right )}$$\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul des $\\alpha_{i,k}$:\n",
    "$$\\begin{align}\n",
    "\\sum_{i = 1}^{n} \\alpha_{i,k}^2 \n",
    "&= \\sum_{i = 1}^{n} \\left(e_k^T (X^T X)^{-1} X_i \\right)\\left(e_k^T (X^T X)^{-1} X_i \\right)^T \\\\\n",
    "&= e_k^T (X^T X)^{-1} \\sum_{i = 1}^{n} X_i  X_i^T (X^T X)^{-1} e_k  \\\\\n",
    "&= e_k^T (X^T X)^{-1} e_k  \\\\\n",
    "&= \\frac 1n e_k^T \\hat{G}^{-1}e_k  \\\\\n",
    "&\\le \\frac{1}{n \\hat{\\lambda_n}}\n",
    "\\end{align}$$\n",
    "\n",
    "$\\hat{\\lambda_n}$ est la valeur propre la plus petite de la matrice de Gram $\\hat{G}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, on a avec probabilité $(1- \\delta)$ : \n",
    "\n",
    "$$ \\vert \\hat{\\theta_{n,k}} - \\theta_k^* \\vert \n",
    "\\le \\sqrt{\\frac{2 \\sigma^2}{n\\hat{\\lambda_n}} log \\left(\\frac 2\\delta\\right )}$$\n",
    "\n",
    "On retrouve dans ce résultat les 3 paramètres définissant la qualité de notre estimateur:\n",
    "- Le bruit $\\sigma$\n",
    "- Le nombre d'échantillons n\n",
    "- Le conditionnement de notre échantillon, par $\\hat{\\lambda_n}$\n",
    "\n",
    "Remarque: si le conditionnement est mauvais (forte colinéarité de certaines variables explicatives), il est possible de combiner ou supprimer des variables. Ce qui entraine une augmentation du bruit pour refléter la perte d'information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle Gaussien\n",
    "\n",
    "Définition : \n",
    "\n",
    "$ Y = X \\theta^* + \\epsilon $ avec : $\\begin{cases}\n",
    "Y &\\in \\mathbb{R}^{n} \\\\\n",
    "X &\\in \\mathbb{R}^{n \\times (p+1)}\n",
    "\\end{cases}$\n",
    "\n",
    "Et $\\epsilon$ tel que $\\epsilon_i$ sont des variables aléatoires $E(\\epsilon) = 0$, indépendantes et __gaussiennes__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemme de Cochran\n",
    "Note : Caractérise la distribution de $\\hat{\\theta}$, base pour les tests statistiques\n",
    "\n",
    "Sous le modèle Gaussien:\n",
    "\n",
    "(i)    $\\hat{\\theta_{n,k}}$ et $\\hat{\\sigma_n}^2$ sont indépendants avec $\\hat{\\sigma_n}^2 = \\frac{1}{n-p-1} \\sum_{i = 1}^n \\hat{\\epsilon_i}^2$\n",
    "\n",
    "(ii)   $\\hat{\\theta_n} - \\theta^* \\sim \\mathcal{N}\\left( 0, (X^T X)^{-1} \\sigma^2 \\right)$\n",
    "\n",
    "(iii)  $\\frac{\\hat{\\sigma_n}^2(n-p-1)}{\\sigma^2} \\sim  \\chi^2(n-p-1)$ (Loi du Chi-2)\n",
    "\n",
    "\n",
    "(iv)  $\\sqrt{\\frac{n}{\\hat{\\sigma_n}^2 S_{n,k}}} (\\hat{\\theta_{n,k}} - \\theta_k^*) \\sim \\mathcal{T}_{n-p-1}$ Loi de Student à (n-p-1) degrés de liberté, avec $S_{n,k} = e_k^T \\hat{G_n} e_k = (X^TX)^{-1}_{k,k}$ (sélection de l'élément k,k)\n",
    "\n",
    "\n",
    "Références:\n",
    "- Lois de Chi-2 (https://fr.wikipedia.org/wiki/Loi_du_χ²)\n",
    "- Lois de Student (https://fr.wikipedia.org/wiki/Loi_de_Student)\n",
    "- Théorème de Cochran : Saporta2006, p.97, application p.297 à l'estimateur biaisé de Var, pages 412-418 pour le modèle gaussien\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lois de $\\chi_2$\n",
    "\n",
    "Soit $Z_1 \\ldots Z_d$ des Gaussiennes centrées réduites\n",
    "\n",
    "$$ Y = \\sum_{i = 0}^d Z_i^2 \\sim \\chi_{d}^2$$\n",
    "\n",
    "### Lois de Student $\\mathcal{T}$\n",
    "\n",
    "Soit $Z \\sim \\mathcal{N}(0, 1)$, $ Y \\sim \\chi_{d}^2$ telles que $ Z \\perp Y $, alors:\n",
    "\n",
    "$$ X = \\frac{Z}{\\sqrt{Y/d}} \\sim \\mathcal{T}_d $$\n",
    "\n",
    "Student à d degrés de libertés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conséquences\n",
    "\n",
    "$$ \\mathbb{P} \\left( \\frac{\\sqrt{n} (\\hat{\\theta_{n,k}} - \\theta^*)}{\\hat{\\sigma_n} \\sqrt{S_{n,k}}} \\in A \\right) = \\Psi(A)$$\n",
    "\n",
    "$\\Psi$ est une fonction connue de loi de Student\n",
    "\n",
    "$$ \\mathbb{P} (a \\le \\ldots \\le b) \n",
    "=   \\mathbb{P} \\left( \\hat{\\theta_{n,k}} - \\frac{\\hat{\\sigma_n} \\sqrt{S_{n,k}}}{\\sqrt{n}}\n",
    "< \\theta^* \n",
    "< \\hat{\\theta_{n,k}} + \\frac{\\hat{\\sigma_n} \\sqrt{S_{n,k}}}{\\sqrt{n}} \\right)$$\n",
    "\n",
    "C'est l'interval de confiance de $\\theta_k^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preuve du Lemme\n",
    "\n",
    "### Preuve de (i)\n",
    "\n",
    "$$ \\hat{\\sigma_n}^2 = \\frac{1}{n - p - 1} \\Vert Y - \\hat{Y} \\Vert_2^2 = \\frac{1}{n - p - 1} \\Vert (I - H_x) \\epsilon \\Vert_2^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ (\\hat{\\theta_n} - \\theta^* ) = (X^T X)^{-1} X^T \\epsilon $$\n",
    "\n",
    "Si $ (X^T \\epsilon) \\perp (I - H_x)\\epsilon$, alors $\\sigma_n^2$ et $(\\hat{\\theta_n} - \\theta^* )$ seront indépendants (par transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme ces grandeurs sont centrées et $I - H_x$ est symmétrique:\n",
    "$$\\begin{align}\n",
    "Cov(X^T \\epsilon, (I - H_x) \\epsilon) \n",
    "&= E(X^T \\epsilon \\epsilon^T (I - H_x)) \\\\\n",
    "&= X^T \\sigma^2 I (I - H_x) \\\\\n",
    "&= \\sigma^2 X^T  (I - H_x) \\\\\n",
    "&= 0\n",
    "\\end{align}$$\n",
    "\n",
    "Car $I - H_x$ est le projeteur sur $\\perp Vect(X) = Ker(X)$\n",
    "\n",
    "On a 2 vecteurs gaussiens décorrelés, ils sont donc indépendants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preuve de (ii)\n",
    "\n",
    "$$\\begin{align}\n",
    "\\sqrt{n} (\\hat{\\theta_n} - \\theta^*) \n",
    "&= \\sqrt{n} (X^T X)^{-1} X^T \\epsilon \\\\\n",
    "&= A \\epsilon\n",
    "\\end{align}$$\n",
    "\n",
    "C'est une transformée linéaire d'un vecteur gaussien, donc gaussien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preuve de (iii)\n",
    "\n",
    "Voir poly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
